{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of reinforce_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DkRt7wrXWOE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# REINFORCE in pytorch\n",
        "\n",
        "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "metadata": {
        "id": "JHX3G_uPWOFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "300be039-4c3d-4fbc-ffdd-5b955ef6e05f"
      },
      "cell_type": "code",
      "source": [
        "# in google colab uncomment this\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "os.system('python -m pip install -U pygame --user')\n",
        "\n",
        "print('setup complete')\n",
        "\n",
        "# XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY = : 1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup complete\n",
            "Starting virtual X frame buffer: Xvfb.\n",
            "env: DISPLAY=: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xk116YmTWOFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "c7ddf500-0a3f-421a-90ef-d728b3d10b3a"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fafc7d9af60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkjplwEtSpwx4SeqUAS9Jnfp3MhuDx0xqxLwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Om98NljPWOFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "metadata": {
        "id": "B9UbjMjfWOFP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
      ]
    },
    {
      "metadata": {
        "id": "YXU0qzMvWOFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaexerAuWvbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "932f83b0-2bbc-42a9-fa19-3743fd8b4a45"
      },
      "cell_type": "code",
      "source": [
        "env.observation_space, env.action_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Box(4,), Discrete(2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "RRhKsQE7WOFT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 2)\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m7WIRw8WWOFW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Predict function"
      ]
    },
    {
      "metadata": {
        "id": "-uxZxz4TWOFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    x = torch.tensor(states, device=device, dtype=torch.float32)\n",
        "    x = model(x)\n",
        "    x = torch.softmax(x, -1)\n",
        "    x = x.detach().cpu().numpy()\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rpwGO2-fWOFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(\n",
        "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (\n",
        "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1),\n",
        "                   1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXaJ2518WOFd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "metadata": {
        "id": "SZOBNgsaWOFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\" \n",
        "    play a full session with REINFORCE agent and train at the session end.\n",
        "    returns sequences of states, actions andrewards\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(range(2), p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZvUkLygWOFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JpaZ-7xWOFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "metadata": {
        "id": "YflgF78HWOFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    gr = [rewards[-1]]\n",
        "    for i in range(len(rewards) - 2, -1, -1):\n",
        "      gr.append(rewards[i] + gamma * gr[-1])\n",
        "    return list(reversed(gr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Kg9lNN1WOFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99817c77-d02d-4260-c27c-6c17e118e502"
      },
      "cell_type": "code",
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9aP6trtOWOFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
      ]
    },
    {
      "metadata": {
        "id": "wPq3Bi2WWOFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).to(device).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(y_tensor.size()[0], ndims, device=device)\n",
        "    y_one_hot = y_one_hot.scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7w3WVdpkWOFr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your code: define optimizers\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32, device=device)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, \n",
        "                                      dtype=torch.float32, device=device)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    entropy = -(probs * log_probs).sum(-1).mean()\n",
        "    loss = -(log_probs_for_actions * cumulative_returns).mean() \\\n",
        "      - entropy_coef * entropy\n",
        "\n",
        "    # Gradient descent step\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-tnD4lEWOFt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The actual training"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_jQdw8kfWOFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6400a5ad-9472-455a-f156-474303fb582b"
      },
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session())\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:18.980\n",
            "mean reward:18.100\n",
            "mean reward:20.490\n",
            "mean reward:21.120\n",
            "mean reward:23.350\n",
            "mean reward:31.580\n",
            "mean reward:38.050\n",
            "mean reward:38.130\n",
            "mean reward:49.970\n",
            "mean reward:52.230\n",
            "mean reward:74.970\n",
            "mean reward:99.820\n",
            "mean reward:94.520\n",
            "mean reward:146.460\n",
            "mean reward:196.250\n",
            "mean reward:135.120\n",
            "mean reward:175.580\n",
            "mean reward:189.100\n",
            "mean reward:156.180\n",
            "mean reward:542.240\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6znl1nfQWOFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Video"
      ]
    },
    {
      "metadata": {
        "id": "qS-kD3xJWOFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
        "                           directory=\"videos\", force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PmWmacyHWOFx",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.612.video000000.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": "Not Found"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "daa71fb1-8bbd-4dc7-80cc-5fcd66ec8bcc"
      },
      "cell_type": "code",
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"./videos/openaigym.video.0.612.video000000.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}